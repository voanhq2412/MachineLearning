{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc601e7e-75f1-4894-9209-a18f4ecc4b48",
   "metadata": {},
   "source": [
    "![alt](images/neural_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25c5e7-f4bc-4a2c-a74b-cefe6a9907d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "A Neural Network is simply a combination of multiple layers of Logistic Regression, with varying activation functions for each layer. \n",
    "\n",
    "- An activation function decides whether a neuron (or node) should be activated or not based on whether its input is important for making the final prediction.\n",
    "- An activation function adds non-linearity to the network so that it can solve complex problems by being able to approximate any continous function. A neural network without activation functions is just a linear regression model.\n",
    "- The activation functions and the weights of the hidden layers will transform the input features and output the final results.\n",
    "\n",
    "\n",
    "#### Activation Functions\n",
    "<img src=\"images/sigmoid.jpg\" style=\"width: 400px;\"/>  <img src=\"images/sigmoid_gradient.jpg\" style=\"width: 360px;\"/> \n",
    "- Because output ranges between 0 and 1, commonly used for predicting probabilities or for binary classification.\n",
    "- The gradient approaches 0 on the sides, suffers from the vanishing gradient problem.\n",
    "- Not symmetrical around zero. This makes the training of the neural network more difficult.\n",
    "- Its variation the Softmax is commonly used in the last layer for multi-class classification. \n",
    "\n",
    "<img src=\"images/tanh.jpg\" style=\"width: 400px;\"/>  <img src=\"images/tanh_gradient.jpg\" style=\"width: 390px;\"/> \n",
    "- Output is zero-centered; output values can be mapped as strongly negative, neutral, or strongly positive.\n",
    "- Sigmoid/Tanh should not be used in deep hidden layers as they are prone to vanishing gradients.\n",
    "\n",
    "<img src=\"images/relu.jpg\" style=\"width: 400px;\"/>  <img src=\"images/relu_gradient.jpg\" style=\"width: 400px;\"/> \n",
    "- ReLU accelerates the convergence of gradient descent.\n",
    "- Gradient value of zero on the negative side creates dead neurons which never get activated. \n",
    "- Should only be used in the hidden layers.\n",
    "\n",
    "<img src=\"images/leaky_relu.jpg\" style=\"width: 400px;\"/> <img src=\"images/swish.jpg\" style=\"width: 400px;\"/>\n",
    "- Leaky Relu enables backpropagation, even for negative input values.\n",
    "- The gradient for negative values is a small value that makes learning time-consuming.\n",
    "- Swish function is used in neural networks having a depth greater than 40 layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92cd654-b834-45ab-ba11-fd4b8d36eaeb",
   "metadata": {},
   "source": [
    "etc. For a 3 layer Neural Network (1 input layer, 1 hidden layer, 1 output layer) we'd have ... <br>\n",
    "$\\sigma(\\hat{Y}_{NxC}) = \\sigma(X_{NxM}W_{MxC}) \\ \\ \\ \\ $input layer to hidden layer (sigmoid activation)<br>\n",
    "$s(\\hat{Z}_{NxK})  = s(\\sigma(\\hat{Y}_{NxC})V_{CxK}) \\ \\ \\ \\ \\ \\ \\ \\ $hidden layer to output layer (softmax activation)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81edfa-2316-4a23-b96c-2d01006bd7ff",
   "metadata": {},
   "source": [
    "CrossEntropy ... <br>\n",
    "J = $-\\dfrac{1}{N} \\sum^{N} \\sum^{K}[z_{nk}log(s({\\hat{z}_{nk}}))]$<br>\n",
    "J = $-\\dfrac{1}{N} \\sum^{N} \\sum^{K}[z_{nk} log(s(\\sigma(\\hat{y}_{nc})v_{ck}))]$<br>\n",
    "J = $-\\dfrac{1}{N} \\sum^{N} \\sum^{K}[z_{nk} log(s(\\sigma(x_{nm}w_{mk})v_{ck}))]$\n",
    "\n",
    "$\\dfrac{dJ}{dw_{mc}} = \\dfrac{-1}{N} \\sum^{N}_{n=1} \\sum^{K}_{k=1} \\dfrac{d(z_{nk}log(s({\\hat{z}_{nk}})))}{ds(\\hat{z}_{nk})} \\dfrac{ds(\\hat{z}_{nk})}{d\\hat{z}_{nj}} \\dfrac{d\\hat{z}_{nj}}{d(\\sigma(x_{nm}w_{mc}))} \\dfrac{d(\\sigma(x_{nm}w_{mc})}{d(x_{nm}w_{mc})} \\dfrac{d(x_{nm}w_{mc})}{dw_{mc}} $\n",
    "\n",
    "$\\dfrac{dJ}{dw_{mc}} = \\dfrac{-1}{N} \\sum^{N}_{n=1} [z_{nj}-s(\\hat{z}_{nj})] v_{cj} \\sigma(x_{nm}w_{mc})[1-\\sigma(x_{nm}w_{mc})]x_{nm}$\n",
    "\n",
    "In matrix form:<br>\n",
    "$\\dfrac{dJ}{dw} = \\dfrac{1}{N} \\sum^{N}_{n=1} \\sum^{K}_{j=1} \\sum^{M}_{m=1} \\sum^{C}_{c=1} [s(\\hat{z}_{nj})-z_{nj}] v_{cj} \\sigma(\\hat{y}_{nc})[1-\\sigma(\\hat{y}_{nc})] x_{nm}$\n",
    "\n",
    "$\\dfrac{dJ}{dw} =  \\dfrac{1}{N} X^{T}_{MxN} \\big[ [s(\\hat{Z})-Z]V^{T} \\odot \\sigma(\\hat{Y}) \\odot [1-\\sigma(\\hat{Y})] \\big]_{NxC} $ where $\\odot$ stands for element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f494d5-2a9b-4185-a896-4dfc07414381",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Different features of the input data may have different scales, one could range in the thousands while another could range from 0-10. Scaling the input may improve performance of machine learning models. It can also help with the vanishing gradients of activation functions like the sigmoid. \n",
    "\n",
    "**Min-max scaling** (or normalization) involes shifting the values such that they end up ranging between 0 and 1. It is done by subtracting the min value, and dividing by the max minus the min.\n",
    "\n",
    "**Standardization** involes fitting a normal distribution over the input data. It is done by subtracting the mean value then dividing by the standard deviation.\n",
    "\n",
    "### Epochs\n",
    "\n",
    "The number of Epochs represent the number of times the learning algorithm will work through the entire training set. The more complex the problem/model the higher the number of epochs should be, so the gradient descent algorithm will have a better chance to converge to the global minimum. \n",
    "\n",
    "### Weights Initialization\n",
    "\n",
    "The initial weights should neither be too high nor too low, else we may encounter vanishing gradients when plugged into functions like the sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f55971f-2dce-4b0f-a421-562bfd751d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output classes: 10\n",
      "There are 60000 observations in the test set, each is a 28 x 28 image\n",
      "Each obsevation should be represented as a vector, so we should flatten each image\n",
      "The pixel intensities range from 0 to 255, it is always a good idea to scale the input values such that they range from 0 to 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    " \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "K = np.max(Y_train)+1\n",
    "print(\"Number of output classes:\",K)\n",
    "# one hot encode\n",
    "Y_train = keras.utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.to_categorical(Y_test)\n",
    "\n",
    "N,w,h = X_train.shape\n",
    "print(\"There are\", N ,'observations in the test set, each is a', w,'x',h,'image')\n",
    "print(\"Each obsevation should be represented as a vector, so we should flatten each image\")\n",
    "print(\"The pixel intensities range from 0 to 255, it is always a good idea to scale the input values such that they range from 0 to 1\")\n",
    "X_train = X_train.reshape((N,-1))\n",
    "X_train = X_train/255\n",
    "bias = np.ones((X_train.shape[0],1))\n",
    "X_train = np.hstack((bias,X_train))\n",
    "\n",
    "X_test= X_test.reshape((X_test.shape[0],-1))\n",
    "X_test = X_test/255\n",
    "bias = np.ones((X_test.shape[0],1))\n",
    "X_test = np.hstack((bias,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff49eb7c-6a01-49ab-b13a-15ba194c49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "W= -1526.1314239348037\n",
      "V= -47.45581055906597\n",
      "\n",
      "\n",
      "\n",
      "Epoch # 1\n",
      "W= -1734.3514829131282\n",
      "V= -47.455810558897745\n",
      "\n",
      "\n",
      "\n",
      "Epoch # 2\n",
      "W= -1878.3105057031912\n",
      "V= -47.45581055873485\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(Y):\n",
    "    return 1/(1+np.exp(-Y))\n",
    "\n",
    "def softmax(Y):\n",
    "    expY = np.exp(Y)\n",
    "    return expY/expY.sum(axis=1,keepdims=True)\n",
    "\n",
    "C = 400\n",
    "lr = 0.01\n",
    "W = np.random.normal(size=(X_train.shape[1],C))\n",
    "V = np.random.normal(size=(C,K))\n",
    "\n",
    "i= 0\n",
    "while i<N:\n",
    "    X_r = X_train[i]\n",
    "    Y_r = Y_train[i]\n",
    "    X_r = X_r.reshape((1,-1))\n",
    "\n",
    "    S = sigmoid(X_r.dot(W))\n",
    "    Z = softmax(S.dot(V))\n",
    "    gradients_W = X_r.T.dot(((Z-Y_r).dot(V.T))*S*(1-S))\n",
    "    gradients_V = S.T.dot(Z-Y_r)\n",
    "\n",
    "    W -= lr*gradients_W\n",
    "    V -= lr*gradients_V\n",
    "    i+=1\n",
    "print(\"W=\",W.sum())\n",
    "print(\"V=\",V.sum())\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2026f5-4bd4-4dac-977f-fb83d7396db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8242"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "S = sigmoid(X_test.dot(W))\n",
    "Z = softmax(S.dot(V))\n",
    "Z = np.floor(Z/np.max(Z,axis=1)[:,None])\n",
    "\n",
    "accuracy_score(Y_test, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1abcbe-1d67-4cb6-bfd6-8730c2c00cbf",
   "metadata": {},
   "source": [
    "The accuracy rate of our model is not that great. Lets try to implement a similar model with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e0494b-7954-4f85-9c94-a3c90f8edd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80730023-95bd-47fb-b9ce-fed68196014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 400)               314000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                4010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318,010\n",
      "Trainable params: 318,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model, number of layers, number of nodes and activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(C,activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(K,activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dffa9d57-846b-467e-ab65-4cabb089672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'SGD', 'learning_rate': 0.01, 'decay': 0.0, 'momentum': 0.0, 'nesterov': False}\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 1.1684 - accuracy: 0.6790\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.7191 - accuracy: 0.7590\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.6294 - accuracy: 0.7827\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.7798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6181872487068176, 0.7797999978065491]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify loss function and optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.optimizer.get_config())\n",
    "\n",
    "## Fit and eval model\n",
    "history = model.fit(X_train,Y_train)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6312b3b-8a23-454a-9228-bbb234a77115",
   "metadata": {},
   "source": [
    "Trying a deeper model with relu activation functions and 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7382c1bc-8792-4224-a326-b0a8219c7bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.6887 - accuracy: 0.7710\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4672 - accuracy: 0.8367\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4204 - accuracy: 0.8515\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3897 - accuracy: 0.8624\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3680 - accuracy: 0.8680\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.3513 - accuracy: 0.8744\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3360 - accuracy: 0.8785\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3247 - accuracy: 0.8828\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3131 - accuracy: 0.8860\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3010 - accuracy: 0.8908\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2931 - accuracy: 0.8940\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2845 - accuracy: 0.8962\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2770 - accuracy: 0.8986\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2685 - accuracy: 0.9014\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2604 - accuracy: 0.9045\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3340 - accuracy: 0.8804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3339943587779999, 0.8804000020027161]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model, number of layers, number of nodes and activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(500,activation=\"relu\"),\n",
    "    keras.layers.Dense(300,activation=\"relu\"),\n",
    "    keras.layers.Dense(100,activation=\"relu\"),\n",
    "    keras.layers.Dense(K,activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Specify loss function and optimizer\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "## Fit and eval model\n",
    "history = model.fit(X_train,Y_train,epochs=15)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22443f0c-f4fd-4c19-a9ed-65d94cb484d8",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning\n",
    "\n",
    "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparamateres to tweak. Tweaking the parameters manually or randomly are not encouraged. Fortunately, there are some Python libraries that can be used to optimize hyperparameters.\n",
    "\n",
    "### Number of Hidden Layers\n",
    "For many problems, you can begin with a single hidden layer and get reasonable results. However, for complex problems, deeper networks can reach much better performance, but they may take very long to train and require a lot of data. \n",
    "\n",
    "It is much common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be a lot faster and require much less data\n",
    "\n",
    "### Number of Neurons per Hidden Layer\n",
    "For the hidden layers, it is best to have the same number of neurons for all hidden layers so that we only have to tweak this one value. \n",
    "\n",
    "In practice, itâ€™s often simpler and more efficient to pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent overfitting.\n",
    "\n",
    "### Others\n",
    "- In general ReLU will be a good default for all hidden layers. \n",
    "- The number of iterations (epochs) does not need to be tweaked, just use early stopping instead. \n",
    "- In general, small batch sizes should be used (i.e. less that 32). Large batch sizes should be used only in conjunction with learning rate warmup.\n",
    "- Picking the learning rate and the optimizer is also important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4055e-7201-4abc-a8b1-f6ce9d768199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
