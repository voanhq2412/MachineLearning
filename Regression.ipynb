{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa130801-01ea-4804-8837-767dc5f324ae",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "A linear model makes a prediction by computing a weighted sum of the input features. Given N observations from the training set, each observation has M features and each feature would be assigned a weight. \n",
    "\n",
    "Training a Linear Regression means finding the weights that minimize some measure of error. The most common performance measure of a regression model is the Mean Square Error (MSE).\n",
    "\n",
    "$\\begin{bmatrix}\\hat{y_{1}}\\\\\\hat{y_{2}}\\\\...\\\\\\hat{y_{N}}\\end{bmatrix}$ = $\\begin{bmatrix}x_{1,1}   x_{1,2} ... x_{1,M}\\\\x_{2,1}   x_{2,2} ... x_{2,M}\\\\...\\\\x_{N,1} x_{N,2} ... x_{N,M}\\end{bmatrix}$ \n",
    "$\\begin{bmatrix}w_{1}\\\\w_{2}\\\\...\\\\w_{M}\\end{bmatrix}$\n",
    "\n",
    "$Y = X.W$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516840d4-358b-4505-8021-47664650cff5",
   "metadata": {},
   "source": [
    "MSE = $\\dfrac{1}{N} \\sum^{N} {(y_{n}-\\hat{y_{n}})}^2$\n",
    "\n",
    "We minimize the MSE by differentiating it against each of the weights, and setting the derivative equal to zero.<br>\n",
    "MSE = $\\dfrac{1}{N} \\sum^{N} {(y_{n}-\\sum^{M} w_{m} x_{nm}   )}^2$ <br>\n",
    "\n",
    "$\\dfrac{d(MSE)}{dw_{k}}$ = $\\dfrac{1}{N} \\sum^{N} \\dfrac{d{(y_{n}-\\sum^{M} w_{m} x_{nm})}^2}{dw_{k}}$\n",
    "\n",
    "Using chain rule $\\dfrac{dz}{dx} = \\dfrac{dz}{dy} . \\dfrac{dy}{dx}$, and setting the derivative to 0, we have ...<br>\n",
    "$\\dfrac{d(MSE)}{dw_{k}}$ = $\\sum^{N} \\dfrac{d{(y_{n}-\\sum^{M} w_{m} x_{nm})}^2}{d(\\sum^{M} w_{m} x_{nm})} . \\dfrac{d(\\sum^{M} w_{m} x_{nm})}{dw_{k}} = 0$ <br><br>\n",
    "\n",
    "$\\sum^{N} -2{(y_{n}-\\sum^{M} w_{m} x_{nm}   )}.x_{nk}$ = 0 <br>\n",
    "$\\sum^{N} y_{n}.x_{nk} =\\sum^{N}\\sum^{M} w_{m} x_{nm}.x_{nk}$<br><br>\n",
    "$\\sum^{N}_{n=1}\\sum^{M}_{k=1} y_{n}.x_{nk} =\\sum^{N}_{n=1}\\sum^{M}_{k=1}\\sum^{M}_{m=1}  w_{m} x_{nm}.x_{nk}$<br><br>\n",
    "\n",
    "In Matrix form:<br>\n",
    "$Y^{T}X = W^{T}{(X^{T}.X)}$<br>\n",
    "$W^{T} = {(X^{T}.X)}^{-1}(Y^{T}X)$<br>\n",
    "$W = {(X^{T}.X)}^{-1}(X^{T}Y)$\n",
    "\n",
    "The above is called the **NORMAL EQUATION**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319e3716-71ac-47cf-b1a1-a5cb429b59a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46191241]\n",
      " [2.01666793]\n",
      " [2.96985048]]\n",
      "[[22.35949035]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9980040612475777"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Example \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys,os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "data = pd.read_csv(\"./machine_learning_examples/linear_regression_class/data_2d.csv\",header=None)\n",
    "X = data.iloc[:,0:2].values\n",
    "Y = data.iloc[:,2].values\n",
    "Y = np.reshape(Y, (Y.shape[0],-1))\n",
    "\n",
    "bias = np.ones((X.shape[0],1))\n",
    "X = np.hstack((bias,X))\n",
    "W = (np.linalg.inv((X.T).dot(X))).dot((X.T).dot(Y))\n",
    "Y_pred = X.dot(W)\n",
    "print(W)\n",
    "\n",
    "MSE = ((Y.T).dot(Y) + (Y_pred.T).dot(Y_pred) - 2*(Y.T).dot(Y_pred))/X.shape[0]\n",
    "print(MSE)\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27559a65-bda1-420d-80b5-1e0f43428987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.46191241]), array([[2.01666793, 2.96985048]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X[:,1:],Y)\n",
    "lin_reg.intercept_,lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6d7c0-84ea-491f-929c-16643eee67a9",
   "metadata": {},
   "source": [
    "By default, sklearn LinearRegressor automatically adds the bias terms to the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2a3b8-da92-4e20-8d41-213b7288b0a1",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is a better approach for Linear Regression models with a large number of features and many training instances. \n",
    "\n",
    "Gradient descent firstly picks a random point of initialization and moves in the direction of descending gradient iteratively until the minimum is reached. The size of the steps is called the learning rate hyperparameter and should be large enough to ensure quick convergence but not too large as to jump across and miss the minimum. \n",
    "\n",
    "For functions where there are many alleys, depending on the initialization point, the GD may converge to a local minimum instead of a global minimum, which is not ideal. Fortunately, the MSE for a LR is a convex function and convergence to a global minimum is guaranteed.\n",
    "\n",
    "![alt](images/gradient_descent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74599741-ef6d-4dd9-9de9-f5f5528ab0cc",
   "metadata": {},
   "source": [
    "Obtain the partial derivatives for the MSE with respect to each weight...\n",
    "\n",
    "$\\dfrac{d(MSE)}{dw_{k}} = \\dfrac{2}{N} \\sum^{N} {(\\sum^{M} w_{m} x_{nm}-y_{n})}.x_{nk}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a8ab1-380a-4b8d-8dab-87bd7f2425c6",
   "metadata": {},
   "source": [
    "$\\dfrac{d(MSE)}{dw} = \\sum^{M}_{k=1} \\dfrac{d(MSE)}{dw_{k}} = \\dfrac{2}{N} [\\sum^{N}_{n=1}\\sum^{M}_{k=1}\\sum^{M}_{m=1}  w_{m} x_{nm} x_{nk}-\\sum^{N}_{n=1}\\sum^{M}_{k=1} y_{n}.x_{nk}]$<br>\n",
    "\n",
    "In Matrix form:<br>\n",
    "$\\dfrac{2}{N} [W^{T}(X^{T}.X) - Y^{T}.X] = \\dfrac{2}{N} [(W^{T}X^{T}).X - Y^{T}.X]$ <br>\n",
    "$\\dfrac{2}{N} [(W^{T}X^{T} - Y^{T}).X]= \\dfrac{2}{N} [(X.W - Y)^{T}.X]$<br><br>\n",
    "$\\dfrac{d(MSE)}{dw} = \\dfrac{2}{N} [X^{T}(X.W - Y)]$ ... is the gradient vector<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744e566-c71e-4758-a27c-5d787d6255a1",
   "metadata": {},
   "source": [
    "If the gradient vector is positive, we go in the opposite direction to go downhill...\n",
    "\n",
    "$w^{next} = w - \\lambda \\dfrac{d(MSE)}{dw}$, where $\\lambda$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4319495-4bbc-4c7c-a732-4a5d0c7580e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [[0.93433524]\n",
      " [0.32665939]\n",
      " [0.73010235]]\n",
      "Gradient= [-50486.81091261]\n",
      "\n",
      "\n",
      "W= [[1.46177305]\n",
      " [2.01666901]\n",
      " [2.9698517 ]]\n",
      "Gradient= [-3.73488206e-05]\n"
     ]
    }
   ],
   "source": [
    "# Initialize random weights\n",
    "W = np.random.rand(X.shape[1],1)\n",
    "gradient_vector = (X.T.dot(X.dot(W)-Y))*2/Y.shape[0]\n",
    "print(\"Starting point:\",W)\n",
    "print(\"Gradient=\",sum(gradient_vector))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.00015\n",
    "iterations = 0\n",
    "# Batch gradient Descent, training on the whole dataset\n",
    "while iterations<200000:\n",
    "    W = W - lr*gradient_vector\n",
    "    gradient_vector = (X.T.dot(X.dot(W)-Y))*2/Y.shape[0]\n",
    "    iterations+=1\n",
    "    \n",
    "print(\"W=\",W)\n",
    "print(\"Gradient=\",sum(gradient_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd1d90-bef0-49b8-b94c-3ea126013696",
   "metadata": {},
   "source": [
    "The results obtained with GD gets very close to that of the Normal Equation. Given enough time, it will reach the optimal. But from this point on it will take even longer, because the rate of convergence decreases as we get closer to the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df002761-78da-4d11-b1b4-213c7a300096",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "When the training set (N) is large and there are many features (M) the gradient descent algorithm could become slow because at every iteration ... computation is done on vectors of size Mx1 and matrices of size MxN. \n",
    "\n",
    "Stochastic Gradient Descent is faster because at each iteration it picks a random instance from the training set and computes the gradients based on that one instance. \n",
    "\n",
    "Another variation is the Mini-batch Gradient Descent, which obtain the gradients based on a small random sample of instances. The main advantage of Mini-batch GD over SGD is that it gets a performance boost from hardware optimized matrix operations, especially when using GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf42318-f1a4-4142-807a-697d1d7a03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= [[0.39456283]\n",
      " [1.96388006]\n",
      " [3.02439826]]\n",
      "Gradient= [0.04842737]\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch SGD\n",
    "lr = 0.00015\n",
    "iterations = 0\n",
    "W = np.random.rand(X.shape[1],1)\n",
    "\n",
    "# We can add a threshold value of 0.05, to stop early once the gradient gets close to 0\n",
    "while iterations<200000:\n",
    "    r = np.random.randint(Y.shape[0])\n",
    "    X_r = X[r:r+5]\n",
    "    Y_r = Y[r:r+5]\n",
    "    X_r = np.reshape(X_r, (X_r.shape[0],-1))\n",
    "    \n",
    "    gradient_vector = (X_r.T.dot(X_r.dot(W)-Y_r))*2/Y_r.shape[0]\n",
    "    if (0 < sum(gradient_vector)) and (sum(gradient_vector) < 0.05):\n",
    "        break \n",
    "    W = W - lr*gradient_vector\n",
    "    iterations+=1\n",
    "    \n",
    "print(\"W=\",W)\n",
    "print(\"Gradient=\",sum(gradient_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697d379-b5b9-46b0-9753-421d313bdc85",
   "metadata": {},
   "source": [
    "# Regularized Linear Regression\n",
    "\n",
    "To avoid overfitting in linear regression, we need to regularize the weights of the model. This means adding a term to the cost function that penalizes large weights.<br>\n",
    "\n",
    "The hyperparameter $\\alpha$ controls how much to regularize the model. The bias term $w_{1}$ is not regularized. <br>\n",
    "\n",
    "\n",
    "#### Ridge Regression (L1)\n",
    "Cost = $\\dfrac{1}{N} \\sum^{N} {(y_{n}-\\hat{y_{n}})}^2 + \\dfrac{\\alpha}{2} \\sum^{N}_{i=2} w^{2}_{i}$ <br>\n",
    "Ridge regression keeps all features but shrinks their weights to helps to reduce the model complexity and multi-collinearity.<br> \n",
    " \n",
    "\n",
    "#### Lasso Regression (L2)\n",
    "Cost = $\\dfrac{1}{N} \\sum^{N} {(y_{n}-\\hat{y_{n}})}^2 + \\alpha \\sum^{N}_{i=2} |w_{i}|$ <br>\n",
    "Lasso regression not only helps in reducing over-fitting but it can help in feature selection and it outputs a sparse model, unimportant predictors are left out of the model (i.e weight = 0). <br> \n",
    "\n",
    "#### ElasticNet \n",
    "Cost = $\\dfrac{1}{N} \\sum^{N} {(y_{n}-\\hat{y_{n}})}^2 + \\dfrac{\\alpha(1-r)}{2} \\sum^{N}_{i=2} w^{2}_{i} + r\\alpha \\sum^{N}_{i=2} |w_{i}|$ <br>\n",
    "\n",
    "\n",
    "It is almost always preferable to have some regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect only a few features are useful then use Lasso or ElasticNet. ElasticNet is better than Lasso when several features are strongly correlated. Similar to plain Linear Regression, Gradient Descent can be used to find the optimal weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543848c3-85b3-47d8-8d97-e4e0f0a14431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04021992]), array([2.02240985, 2.9879615 ]), 0.9979741296257675)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent regressor, with L1 (Ridge) regularization in sklearn\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sdg_reg = SGDRegressor(max_iter=200000,eta0=0.00015,penalty='l1')\n",
    "sdg_reg.fit(X[:,1:],Y.ravel())\n",
    "Y_pred = sdg_reg.predict(X[:,1:])\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "sdg_reg.intercept_,sdg_reg.coef_,r2_score(Y,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419b94e-3fb4-4c3c-96e2-396dd44318f7",
   "metadata": {},
   "source": [
    "\n",
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a binary classification algorithm. It estimates the probability that an instance belongs to a particular class. The probability is rounded to 0 or 1 to obtain the final answer.\n",
    "\n",
    "Logistic Regression model computes the sigmoid of the weighted sum of input features $\\sigma (X_{NxM}.W_{Mx1})$. \n",
    "\n",
    "![alt](images/logistic_function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd852eea-c127-4c8b-a953-6175c18bd888",
   "metadata": {},
   "source": [
    "The objective of training is to set parameter vector W so that the model estimates high probabilities for positive instances (y = 1) and low probabilities for negative instances (y = 0).<br>Large deviations between $y_{n}$ and $\\hat{y_{n}}$ should be penalized.\n",
    "\n",
    "CrossEntropy = ... <br>\n",
    "J = $-\\dfrac{1}{N} \\sum^{N} [y_{n}log(\\sigma(\\hat{y_{n}})) + (1-y_{n})log(1-\\sigma(\\hat{y_{n}}))]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a10459-3103-4bb2-a258-ca4b1bea2415",
   "metadata": {},
   "source": [
    "- If $y_{n} = 1$, the log value approaches $-\\infty$ as $\\hat{y_{n}}$ approaches zero.  \n",
    "- If $y_{n} = 0$, the log value approaches $-\\infty$ as $\\hat{y_{n}}$ approaches 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f3c47-60e9-4c8a-8259-d38a6da9f264",
   "metadata": {},
   "source": [
    "To minimize the cost function, we obtain partial derivatives w.r.t each of the weights.\n",
    "\n",
    "$\\dfrac{dJ}{dw_{k}}$ = $-\\dfrac{1}{N} \\sum^{N} \\dfrac{d(y_{n}log(\\sigma(\\hat{y_{n}})) + (1-y_{n})log(1-\\sigma(\\hat{y_{n}}))])}{d(\\sigma(\\hat{y_{n})})} \\dfrac{d(\\sigma(\\hat{y_{n}}))}{dw_{k}}$\n",
    "\n",
    "$\\dfrac{dJ}{dw_{k}}$ = $-\\dfrac{1}{N} \\sum^{N} \\dfrac{y_{n}(1+e^{-\\hat{y_{n}}})^{2} - (1+e^{-\\hat{y_{n}}})}{e^{-\\hat{y_{n}}}} \\dfrac{d(\\sigma(\\hat{y_{n}}))}{dw_{k}}$\n",
    "\n",
    "$\\dfrac{dJ}{dw_{k}}$ = $-\\dfrac{1}{N} \\sum^{N} \\dfrac{y_{n}(1+e^{-\\hat{y_{n}}})^{2} - (1+e^{-\\hat{y_{n}}})}{e^{-\\hat{y_{n}}}} \\dfrac{d(\\sigma(\\hat{y_{n}}))}{d\\hat{y_{n}}} \\dfrac{{d\\hat{y_{n}}}}{dw_{k}}$\n",
    "\n",
    "$\\dfrac{dJ}{dw_{k}}$ = $-\\dfrac{1}{N} \\sum^{N} \\dfrac{y_{n}(1+e^{-\\hat{y_{n}}})^{2} - (1+e^{-\\hat{y_{n}}})}{e^{-\\hat{y_{n}}}} \\dfrac{e^{-\\hat{y_{n}}}}{(1+e^{-\\hat{y_{n}}})^{2}} x_{nk}$\n",
    "\n",
    "$\\dfrac{dJ}{dw_{k}}$ = $\\dfrac{-1}{N} \\sum^{N} \\left[y_{n} - \\dfrac{1}{(1+e^{-\\hat{y_{n}}})}\\right] x_{nk}$\n",
    "\n",
    "$\\dfrac{dJ}{dw} = \\sum^{M}_{k=1} \\dfrac{dJ}{dw_{k}} = \\dfrac{-1}{N} \\sum^{M}_{k=1} \\sum^{N}_{n=1} \\left[y_{n} - \\dfrac{1}{(1+e^{-\\hat{y_{n}}})}\\right] x_{nk}$\n",
    "\n",
    "In matrix form:\n",
    "$\\dfrac{dJ}{dw} = \\dfrac{1}{N} X^{T}[\\sigma(XW)-Y]$ ... is the gradient vector.<br><br>\n",
    "\n",
    "Similar to Linear Regression, the optimal weights to a Logistic Regression model can be found using Gradient Descent and the model can be regularized using L1 or L2 penalities.\n",
    "\n",
    "Next we implement a Logistic Regression model on the iris dataset, trying to detect the *Iris virginica*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd04d3e9-67bf-4aba-8d6a-8677cf684715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Gradient= [[0.6596706 ]\n",
      " [3.61170314]\n",
      " [2.04354834]\n",
      " [1.8933321 ]\n",
      " [0.52079459]]\n",
      "\n",
      "\n",
      "W= [[-0.10393231]\n",
      " [-0.74857426]\n",
      " [-0.67264665]\n",
      " [ 0.93949494]\n",
      " [ 1.32211633]]\n",
      "Gradient= [-0.01098803]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.feature_names)\n",
    "X = iris.data\n",
    "Y = (iris.target==2).astype(int) # y==2 is iris virginica\n",
    "Y = np.reshape(Y, (Y.shape[0],-1))\n",
    "\n",
    "# Add bias terms and initialize random weights\n",
    "bias = np.ones((X.shape[0],1))\n",
    "X = np.hstack((bias,X.copy()))\n",
    "W = np.random.rand(X.shape[1],1)\n",
    "def sigmoid(X,W):\n",
    "    return 1/(1+np.exp(-(X.dot(W))))\n",
    "\n",
    "gradient_vector = X.T.dot(sigmoid(X,W)-Y)/Y.shape[0]\n",
    "print(\"Gradient=\",gradient_vector)\n",
    "print(\"\\n\")\n",
    "\n",
    "lr = 0.1\n",
    "iterations = 0\n",
    "while iterations<100:\n",
    "    W -= lr*gradient_vector\n",
    "    gradient_vector = X.T.dot(sigmoid(X,W)-Y)/Y.shape[0]\n",
    "    iterations+=1\n",
    "    \n",
    "    \n",
    "print(\"W=\",W)\n",
    "print(\"Gradient=\",sum(gradient_vector))\n",
    "Y_pred = np.round(sigmoid(X,W))\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1be918-adfd-4322-b683-493324c77a61",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Logistic Regression can be generalized to support multiple classes directly. Each class will have its own parameter vector $W_{m}$. The paramter matrix will be of dimension MxK (K classes, M parameters for each)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b7fe9-ab2b-4c9b-a798-fde5d9b8fd10",
   "metadata": {},
   "source": [
    "Softmax Function: <br>\n",
    "$s(\\hat{y}_{nk}) = \\dfrac{e^{\\hat{y}_{nk}}}{\\sum^{K}_{j=1} e^{\\hat{y}_{nj}} }$\n",
    "\n",
    "$\\hat{Y}_{NxK} = X_{NxM}W_{MxK}$ .......... where $y_{nk}$ is the probablity that instance n belongs to class k. <br><br>\n",
    "\n",
    "Given instance n: <br>\n",
    "$\\hat{y}_{n} = \\begin{pmatrix} \\hat{y}_{n1} & \\hat{y}_{n2} &  ...  & \\hat{y}_{nK} \\end{pmatrix} = \n",
    "\\begin{pmatrix} x_{n1} & x_{n2} & ... & x_{nM} \\end{pmatrix}\n",
    "\\begin{pmatrix} w_{11} & ... & w_{1K} \\\\ ... & ... & ... \\\\ w_{M1} & ... & w_{MK} \\end{pmatrix}$  \n",
    "\n",
    "$\\hat{y}_{nj} = \n",
    "\\begin{pmatrix} x_{n1} & x_{n2} & ... & x_{nM} \\end{pmatrix}\n",
    "\\begin{pmatrix} w_{1j} \\\\ ... \\\\ w_{Mj} \\end{pmatrix}$  \n",
    "\n",
    "The final predicted class is the one with the highest probability out of all the classes. <br><br>\n",
    "\n",
    "___\n",
    "\n",
    "The optimal Weight matrix is obtained by minimizing the CrossEntropy ... <br>\n",
    "J = $-\\dfrac{1}{N} \\sum^{N} \\sum^{K}[y_{nk}log(s({\\hat{y}_{nk}}))]$\n",
    "\n",
    "$\\dfrac{dJ}{dw_{mj}} = \\dfrac{-1}{N} \\sum^{N}_{n=1} \\sum^{K}_{k=1} \\dfrac{d(y_{nk}log(s({\\hat{y}_{nk}})))}{ds(\\hat{y}_{nk})} \\dfrac{ds(\\hat{y}_{nk})}{d(\\hat{y}_{nj})} \\dfrac{d(\\hat{y}_{nj})}{dw_{mj}} $\n",
    "- We can differentiate $s(\\hat{y}_{nk})$ w.r.t any $\\hat{y}_{nj}$ for $j \\in K$. <br><br>\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3eec1-4cd6-44d9-af57-3501878cf77b",
   "metadata": {},
   "source": [
    "Partial Derivatives: <br>\n",
    "$\\dfrac{d(y_{nk}log(s({\\hat{y}_{nk}})))}{ds(\\hat{y}_{nk})} = \\dfrac{y_{nk}}{s(\\hat{y}_{nk})}$<br><br>\n",
    "\n",
    "If $k=j...$ $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ If $k \\neq j$...<br>\n",
    "$\\dfrac{ds(\\hat{y}_{nk})}{d(\\hat{y}_{nj})} = s(\\hat{y}_{nk})[1-s(\\hat{y}_{nk})]$ $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  $          $\\dfrac{ds(\\hat{y}_{nk})}{d(\\hat{y}_{nj})} = s(\\hat{y}_{nk})[0-s(\\hat{y}_{nj})]$ \n",
    "\n",
    "Let $\\delta_{jk} = \\begin{cases}\n",
    "      1 & \\text{if k=j}\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$ $\\ \\ \\ \\ \\ => \\ \\ \\ $ $\\dfrac{ds(\\hat{y}_{nk})}{d(\\hat{y}_{nj})} = s(\\hat{y}_{nk})[\\delta_{jk}-s(\\hat{y}_{nj})]$ <br><br>\n",
    "    \n",
    "$\\dfrac{d(\\hat{y}_{nj})}{dw_{mj}} = x_{nm}$<br><br>\n",
    "\n",
    "Thus, we have ...\n",
    "$\\dfrac{dJ}{dw_{mj}} = \\dfrac{-1}{N} \\sum^{N}_{n=1} \\sum^{K}_{k=1} \\dfrac{y_{nk}}{s(\\hat{y}_{nk})} s(\\hat{y}_{nk})[\\delta_{jk}-s(\\hat{y}_{nj})] x_{nm}$ <br><br>\n",
    "\n",
    "$\\sum^{K}_{k=1} y_{nk} [\\delta_{jk}-s(\\hat{y}_{nj})] = \\sum^{K}_{k=1} y_{nk} \\delta_{jk} - \\sum^{K}_{k=1} y_{nk} s(\\hat{y}_{nj}) = y_{nj} \\delta_{jj} -  s(\\hat{y}_{nj}) \\sum^{K}_{k=1} y_{nk} = y_{nj} -  s(\\hat{y}_{nj}) $\n",
    "- Only one target class = 1, the rest should be 0.\n",
    "- $\\delta_{jk}$ = 1 when j=k.\n",
    "\n",
    "$\\dfrac{dJ}{dw_{mj}} = \\dfrac{1}{N} \\sum^{N}_{n=1} [s(\\hat{y}_{nj}) - y_{nj}] x_{nm} $ <br><br>\n",
    "\n",
    "In matrix form:<br>\n",
    "$\\dfrac{dJ}{dw} = \\dfrac{1}{N} \\sum^{K}_{j=1} \\sum^{M}_{m=1} \\sum^{N}_{n=1} [s(\\hat{y}_{nj}) - y_{nj}] x_{nm} = \\dfrac{1}{N} X^{T}[S-Y]$<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020933bc-5cb9-4c96-843d-78f15fa2884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Final W:\n",
      " [[ 1.17885699  2.50512593  5.06738172 -7.09848929 -3.48547412]\n",
      " [ 8.39520276  0.40501534 -0.01029974  0.13094853 -4.06613722]\n",
      " [-9.57405975 -2.91014127 -5.05708197  6.96754076  7.55161134]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "iris = datasets.load_iris()\n",
    "print(iris.feature_names)\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver='sag',penalty='none',max_iter=10000)\n",
    "softmax_reg.fit(X,Y.ravel())\n",
    "Y_pred = softmax_reg.predict(X)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "bias = softmax_reg.intercept_\n",
    "bias = bias.reshape((bias.shape[0],-1))\n",
    "W = np.hstack((bias,softmax_reg.coef_))\n",
    "\n",
    "accuracy_score(Y, Y_pred)\n",
    "print(\"Final W:\\n\",W)\n",
    "accuracy_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ff5437-555c-4b51-98f5-af8c5ec6472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= [[ 1.45230015  0.79545567 -0.8516103 ]\n",
      " [ 1.39075866  1.33232041 -0.98007906]\n",
      " [ 2.61750391  0.50997802 -1.16714139]\n",
      " [-2.32179923  0.27935419  3.26906507]\n",
      " [-1.03453791 -0.69890885  3.09895392]]\n",
      "Gradient= [[-0.00136449 -0.00454682  0.00591131]\n",
      " [-0.00317449 -0.00203603  0.00521052]\n",
      " [-0.00672603 -0.00156329  0.00828933]\n",
      " [ 0.00881019  0.00162103 -0.01043121]\n",
      " [ 0.00423791  0.0052446  -0.00948251]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "# Add bias terms, one-hot-encode, initialize random weights\n",
    "bias = np.ones((X.shape[0],1))\n",
    "X = np.hstack((bias,X.copy()))\n",
    "Y = keras.utils.to_categorical(Y)\n",
    "W = np.random.rand(X.shape[1],Y.shape[1])\n",
    "\n",
    "lr = 0.1\n",
    "iterations = 0\n",
    "def softmax(Y):\n",
    "    expY = np.exp(Y)\n",
    "    return expY/expY.sum(axis=1,keepdims=True)\n",
    "\n",
    "while iterations<1000:\n",
    "    S = softmax(X.dot(W))\n",
    "    gradient_vector = (X.T.dot(S-Y))/Y.shape[0]\n",
    "    W -= lr*gradient_vector\n",
    "    iterations+=1\n",
    "    \n",
    "print(\"W=\",W)\n",
    "print(\"Gradient=\",gradient_vector)\n",
    "\n",
    "Y_pred = X.dot(W)\n",
    "Y_pred = np.floor(Y_pred/np.max(Y_pred,axis=1)[:,None])\n",
    "Y_pred[Y_pred!=1]=0\n",
    "accuracy_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6512616-0e66-4ced-ab9a-2a5f9265cdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a63a3-fc18-4817-8886-3914f7412ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
