{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faf5f81-b449-47a8-b28f-ab1d25c18e85",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees are very versatile, they can perform both regression and classification tasks.\n",
    "\n",
    "Benefits:\n",
    "- Don’t require feature scaling or centering at all. \n",
    "- Easy to interpret than other models.\n",
    "- Make very few assumptions about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2357f7b1-8413-4de1-8077-1223cd7a091f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data[:, 2:]  # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66acc25c-42f2-47b7-9509-8f1d7f005eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "# and convert .dot to png with command..\n",
    "# dot -Tpng iris_tree.dot -o iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c444cab-9cc1-474d-8207-6cb36f24000a",
   "metadata": {},
   "source": [
    "![alt](images/iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f4f84-493d-4e86-b032-dd0ddf0253af",
   "metadata": {},
   "source": [
    "To avoid overfitting the training data, one can at least restrict the maximum depth of the Decision Tree.\n",
    "\n",
    "Other regularization parameters:\n",
    "- min_samples_split: minimum number of samples a node must have before it can be split.\n",
    "- min_samples_leaf (the minimum number of samples a leaf node must have).\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073fe01-9419-4147-9058-49636a6a26e3",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "By aggregating the predictions of a group of predictors (such as classifiers or regressors), one will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble.\n",
    "\n",
    "As an example of an Ensemble method, one can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions,\n",
    "you obtain the predictions of all the individual trees, then predict the class that gets the\n",
    "most votes. Such an ensemble of Decision Trees is\n",
    "called a Random Forest.\n",
    "\n",
    "The following are the most popular ensemble methods:\n",
    "1) Voting Classifiers\n",
    "- Suppose you have trained a few classifiers, each achieving about 80% accuracy. You may have trained a Logistic Regression classifier, a SVM classifier, a Random Forest classifier, and a few more. A prediction from each classifier is considered a vote, and the class with the most votes is the final prediction. \n",
    "2) Average/Weighted Average\n",
    "- For regression problems, one can train multiple models and obtain a weighted average value of their predictions. This weighted value will be the final prediction.\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. One\n",
    "way to get diverse classifiers is to train them using very different algorithms. This increases the chance\n",
    "that they will make very different types of errors, improving the ensemble’s accuracy.\n",
    "\n",
    "3) Bagging and Pasting\n",
    "- Another approach is to use the same training algorithm for every predictor but train them on different random subsets of the training set. When sampling is performed with replacement (the observation is returned to the population before the next one is drawn) it is called **bagging**. When sampling is performed without replacement is it called **pasting**. \n",
    "- With bagging, some instances may be sampled several times while others may not be sampled at all. Those instances that are not sampled for training can be used as our validation set. This is called **Out-of-bag Evaluation**. \n",
    "\n",
    "**Random Forests** is just an ensemble of Decision Trees, generally trained via the bagging (or sometimes pasting) method. Instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. The algorithm results in greater\n",
    "tree diversity.\n",
    "\n",
    "4) AdaBoost\n",
    "- The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d81fbf-0714-4b92-b197-e4692a784b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
